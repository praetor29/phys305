{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Optimization Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Optimization methods find values that maximize or minimize an objective function, making them useful across disciplines such as engineering, economics, and data science.\n",
    "Fundamentally, the action principle in physics is an optimization process, where nature selects paths that minimize or extremize an action integral."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Gradient Descent Methods\n",
    "\n",
    "Gradient Descent is one of the most widely used optimization techniques, particularly effective for high-dimensional problems in fields such as machine learning.\n",
    "The method iteratively seeks the minimum of a function by taking steps proportional to the negative of its gradient, guiding the search toward lower function values.\n",
    "For differentiable objective functions, gradient descent is fundamental in minimizing errors, making it indispensable for training machine learning models and refining physical models in computational astrophysics.\n",
    "\n",
    "For a function $f(x)$, the gradient $\\nabla f(x)$ points in the direction of steepest ascent.\n",
    "Moving in the opposite direction—along the negative gradient—reduces the function's value. The algorithm updates the parameters iteratively according to:\n",
    "\\begin{align}\n",
    "x_{n+1} = x_n - \\alpha \\nabla f(x_n)\n",
    "\\end{align}\n",
    "where $\\alpha$ is the learning rate, controlling the step size.\n",
    "The choice of $\\alpha$ is critical for convergence: \n",
    "a large $\\alpha$ may cause divergence, where updates overshoot the minimum, while a very small $\\alpha$ can lead to slow convergence, requiring many iterations to make meaningful progress.\n",
    "Proper tuning of $\\alpha$ ensures that the algorithm efficiently converges to a minimum without unnecessary oscillations or divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(df, x, alpha, imax=1000):\n",
    "    for _ in range(imax):\n",
    "        x -= alpha * df(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function and its gradient\n",
    "def f(x):\n",
    "    return (x - 3)**2 + 4\n",
    "\n",
    "def df(x):\n",
    "    return 2 * (x - 3)\n",
    "\n",
    "# Parameters for gradient descent\n",
    "x0    = 0.0  # Starting point for optimization\n",
    "alpha = 0.1\n",
    "\n",
    "# Run gradient descent\n",
    "xmin = gd(df, x0, alpha)\n",
    "print(\"Approximate minimum:\")\n",
    "print(\"  xmin  = \",   xmin )\n",
    "print(\"f(xmin) = \", f(xmin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd_hist(df, x, alpha, imax=1000):\n",
    "    X = [x]\n",
    "    for _ in range(imax):\n",
    "        X.append(X[-1] - alpha * df(X[-1]))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "X = np.linspace(0, 6, 6001)\n",
    "plt.plot(X, f(X))\n",
    "\n",
    "alpha = 0.1\n",
    "\n",
    "X = np.array(gd_hist(df, x0, alpha))\n",
    "print(X[-1])\n",
    "\n",
    "plt.plot(X, f(X), '-o')\n",
    "plt.xlim(2.5, 3.5)\n",
    "plt.ylim(3.95,4.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "```{exercise}\n",
    "What will happen if we change the learning rate $\\alpha$?\n",
    "\n",
    "Comment out the plot limits `plt.xlim(2.5, 3.5)` and `plt.ylim(3.95,4.3)` and then try $\\alpha = 0.1$, $0.5$, $0.9$, $1.0$, and $1.1$.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Similar to our implementation of Newton-Raphson Method, it is possible to employ `JAX` to automatically obtain the derivative.\n",
    "Here is an updated version of automatic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import grad\n",
    "\n",
    "def autogd_hist(f, x, alpha, imax=1000):\n",
    "    df = grad(f)\n",
    "    X  = [x]\n",
    "    for _ in range(imax):\n",
    "        X.append(X[-1] - alpha * df(X[-1]))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function and its gradient\n",
    "def f(x):\n",
    "    return (x - 3)**2 + 4\n",
    "\n",
    "# Parameters for gradient descent\n",
    "x0    = 0.0  # Starting point for optimization\n",
    "alpha = 0.9\n",
    "\n",
    "# Run gradient descent\n",
    "Xmin = np.array(autogd_hist(f, x0, alpha))\n",
    "print(\"Approximate minimum:\")\n",
    "print(\"  xmin  = \",   Xmin[-1] )\n",
    "print(\"f(xmin) = \", f(Xmin[-1]))\n",
    "\n",
    "X = np.linspace(0, 6, 6001)\n",
    "plt.plot(X,    f(X))\n",
    "plt.plot(Xmin, f(Xmin), '-o')\n",
    "plt.xlim(2.5, 3.5)\n",
    "plt.ylim(3.95,4.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Gradient Descent with JAX for Multiple Dimensions\n",
    "\n",
    "Multidimensional gradient descent is essential for optimizing functions with multiple parameters, making it the backend of applications such as model fitting and deep learning.\n",
    "\n",
    "In astrophysics, gradient descent refines models by iteratively adjusting parameters to minimize discrepancies between observed data and theoretical predictions.\n",
    "For example, in galaxy modeling, each parameter may correspond to a physical property—such as brightness, size, or position—and gradient descent enables efficient optimization to achieve the best fit to observational data.\n",
    "\n",
    "In deep learning, multidimensional gradient descent is fundamental, as modern neural networks can have millions of parameters.\n",
    "During training, the algorithm minimizes a loss function that quantifies the difference between the model’s predictions and actual outcomes.\n",
    "Automatic differentiation with JAX streamlines gradient calculations, allowing practitioners to train complex models without manually computing derivatives.\n",
    "This capability is particularly valuable for architectures such as convolutional and recurrent neural networks, where gradients must be computed across vast numbers of interconnected parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "The following example demonstrates how to use JAX to perform gradient descent on a multivariable function\n",
    "\\begin{align}\n",
    "f(x, y) = (x - 3)^2 + (y + 4)^2,\n",
    "\\end{align}\n",
    "where the minimum is at $(x, y) = (3, -4)$.\n",
    "By tracking each update step, we can visualize the optimization path as it approaches the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import numpy as jnp\n",
    "from jax import jit\n",
    "\n",
    "# Function to perform gradient descent with history tracking\n",
    "def autogd_hist(f, X, alpha, imax=1000):\n",
    "    df = jit(grad(f))  # Use JAX to compute gradient\n",
    "    Xs = [np.array(X)]\n",
    "    for _ in range(imax):\n",
    "        Xs.append(Xs[-1] - alpha * df(Xs[-1]))  # Gradient descent update\n",
    "    return jnp.array(Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a multivariable function\n",
    "def f(X):\n",
    "    x, y = X\n",
    "    return (x - 3)**2 + 2 * (y + 4)**2\n",
    "\n",
    "# Parameters for gradient descent\n",
    "X0    = jnp.array([0.0, 0.0]) # Starting point for optimization\n",
    "alpha = 0.1                   # Learning rate\n",
    "\n",
    "# Run gradient descent with history tracking\n",
    "Xs = autogd_hist(f, X0, alpha)\n",
    "print(\"Approximate minimum:\")\n",
    "print(\"  xmin  =\",   Xs[-1] )\n",
    "print(\"f(xmin) =\", f(Xs[-1]))\n",
    "\n",
    "# Plot the function and gradient descent path\n",
    "x_vals = jnp.linspace(-1, 7, 100)\n",
    "y_vals = jnp.linspace(-8, 0, 100)\n",
    "X, Y   = jnp.meshgrid(x_vals, y_vals)\n",
    "Z      = f([X, Y])\n",
    "\n",
    "plt.contour(X, Y, Z, levels=20)\n",
    "plt.plot(Xs[:,0], Xs[:,1], '-o', color='red')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.gca().set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Because we minimize $f(x,y)$, it can be seen as the loss function.\n",
    "Hence we can plot the evolution of the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.loglog(f(Xs.T))\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss f(x,y)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "To demonstrate a more complex optimization scenario, let's consider fitting a multi-parameter model to noisy data.\n",
    "We will use polynomial regression as our example, where we fit a polynomial curve to data points by optimizing the coefficients.\n",
    "This is a non-trivial problem because, as the degree of the polynomial increases, the number of parameters grows, resulting in a high-dimensional optimization task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "groundtruth = np.array([1.2, -3, 0.5, 1.0, -1.8, 2.0, -0.1])\n",
    "\n",
    "Xdata = np.linspace(-1, 1, 1_000)\n",
    "Ytrue = sum(c * Xdata**i for i, c in enumerate(groundtruth))\n",
    "Ydata = Ytrue + np.random.normal(scale=0.1, size=Xdata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Xdata, Ydata)\n",
    "plt.plot(Xdata, Ytrue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define polynomial model\n",
    "def model(Xs, Cs):\n",
    "    return sum(c * Xs**i for i, c in enumerate(Cs))\n",
    "\n",
    "# Define the objective function\n",
    "def chi2(Cs):\n",
    "    Ymodel = model(Xdata, Cs)\n",
    "    return jnp.mean((Ymodel - Ydata)**2)\n",
    "\n",
    "# Parameters for gradient descent\n",
    "C0    = jnp.zeros(len(groundtruth)) # Start with zeros as initial coefficients\n",
    "alpha = 0.6                         # Learning rate\n",
    "\n",
    "Cs = autogd_hist(chi2, C0, alpha)\n",
    "%timeit -r1 Cs = autogd_hist(chi2, C0, alpha)\n",
    "\n",
    "print(\"Optimized coefficients:\", Cs[-1])\n",
    "print(\"True coefficients:\",      groundtruth)\n",
    "print(\"Mean Squared Error:\",     np.mean((groundtruth - Cs[-1])**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "To visualize the fitting process, we plot the history every skip steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkplot(Xdata, Ydata, Ytrue, Cs, skip=10):\n",
    "    imax = len(Cs)-1\n",
    "\n",
    "    fig, (ax0, ax1) = plt.subplots(1,2,figsize=(12,6))\n",
    "    \n",
    "    ax0.scatter(Xdata[::skip], Ydata[::skip], color='blue', label='Noisy Data', alpha=0.5)\n",
    "    for i, Ci in enumerate(Cs[::skip]):\n",
    "        Yfit = model(Xdata, Ci)\n",
    "        ax0.plot(Xdata, Yfit, 'r', alpha=skip*i/imax, label='Fitted Polynomial' if skip*i == imax else '')\n",
    "    ax0.plot(Xdata, Ytrue, 'g--', label='True Polynomial')\n",
    "    ax0.set_xlabel(\"x\")\n",
    "    ax0.set_ylabel(\"y\")\n",
    "    ax0.legend()\n",
    "\n",
    "    Chi2 = [chi2(Ci) for Ci in Cs]\n",
    "    ax1.loglog(Chi2, 'o-')\n",
    "    ax1.set_xlabel('Step')\n",
    "    ax1.set_ylabel(r'$\\chi^2$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkplot(Xdata, Ydata, Ytrue, Cs, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Stochastic Gradient Descent (SGD) is a widely used optimization technique, especially valuable for high-dimensional and large-scale datasets.\n",
    "In traditional gradient descent, each parameter update involves computing the gradient over the entire dataset, which can be computationally intensive.\n",
    "In contrast, SGD updates parameters based on a randomly selected subset (or \"batch\") of data points in each iteration.\n",
    "This approach has several advantages:\n",
    "1. Efficiency: By using a smaller batch of data, SGD significantly reduces computation time per iteration, allowing faster updates.\n",
    "2. Memory Management: Processing smaller batches of data at a time is less memory-intensive, making SGD scalable for large datasets.\n",
    "3. Avoiding Local Minima: The randomness introduced in each update step can help the optimizer escape local minima, as it prevents the algorithm from settling into small dips in the landscape.\n",
    "\n",
    "SGD is particularly valuable in machine learning, where models have a large number of parameters and datasets are extensive.\n",
    "For example, in deep learning, SGD enables the efficient training of models with millions of parameters by adjusting weights based on mini-batches, allowing faster convergence with less computational burden.\n",
    "In scientific research areas like astronomy, SGD is useful for optimizing parameters in models that analyze large survey data. By applying mini-batch SGD, researchers can fit complex models to data efficiently, even in cases with high-dimensional parameter spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.random.permutation(len(Xdata))\n",
    "Xrand = Xdata[p]\n",
    "Yrand = Ydata[p]\n",
    "\n",
    "# Define the batch MSE loss function\n",
    "def chi2_batch(Cs, Xbatch, Ybatch):\n",
    "    Ymodel = model(Xbatch, Cs)\n",
    "    return jnp.mean((Ymodel - Ybatch)**2)\n",
    "\n",
    "# Function to perform gradient descent with history tracking\n",
    "def sgd_hist(f, X, alpha, batch_size=1000, imax=1000):\n",
    "    df = jit(grad(f))  # Use JAX to compute gradient\n",
    "    Xs = [np.array(X)]\n",
    "    for i in range(imax):\n",
    "        j = i % (len(Xdata) // batch_size)\n",
    "        Xbatch = Xrand[j*batch_size:(j+1)*batch_size]\n",
    "        Ybatch = Yrand[j*batch_size:(j+1)*batch_size]\n",
    "        Xs.append(Xs[-1] - alpha * df(Xs[-1], Xbatch, Ybatch))  # Gradient descent update\n",
    "    return jnp.array(Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for gradient descent\n",
    "C0    = jnp.zeros(len(groundtruth)) # Start with zeros as initial coefficients\n",
    "alpha = 0.1                         # Learning rate\n",
    "\n",
    "Cs = sgd_hist(chi2_batch, C0, alpha, 100)\n",
    "%timeit -r1 Cs = sgd_hist(chi2_batch, C0, alpha, 100)\n",
    "\n",
    "print(\"Optimized coefficients:\", Cs[-1])\n",
    "print(\"True coefficients:\",      groundtruth)\n",
    "print(\"Mean Squared Error:\",     np.mean((groundtruth - Cs[-1])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkplot(Xdata, Ydata, Ytrue, Cs, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### The Adam Optimizer\n",
    "\n",
    "The Adam optimizer (short for Adaptive Moment Estimation) is a popular optimization algorithm that combines features of both momentum and adaptive learning rates.\n",
    "Introduced in the paper [\"Adam: A Method for Stochastic Optimization\" by Kingma and Ba](https://arxiv.org/abs/1412.6980), Adam has since become one of the most widely used optimizers in machine learning and deep learning.\n",
    "Its efficiency, robustness, and ease of use make it particularly suited for training large neural networks and handling high-dimensional parameter spaces.\n",
    "\n",
    "Adam builds on two primary ideas:\n",
    "\n",
    "1. Momentum: Like momentum-based optimization methods, Adam incorporates a moving average of past gradients, which helps smooth the updates and accelerates convergence in directions with consistent gradients.\n",
    "2. Adaptive Learning Rates: Adam adapts the learning rate for each parameter individually, based on the historical gradient information. This feature allows it to adjust step sizes dynamically, making it more effective on functions with non-uniform gradient scales.\n",
    "\n",
    "By combining these elements, Adam achieves faster and more stable convergence than traditional stochastic gradient descent (SGD), especially for noisy and sparse data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "The Adam optimizer maintains two moving averages for each parameter:\n",
    "\n",
    "* First moment ($m$): This tracks the average of the gradients, effectively capturing the direction and smoothing the gradient signal.\n",
    "* Second moment ($v$): This tracks the average of the squared gradients, allowing the algorithm to adapt the learning rate based on the variance of gradients.\n",
    "\n",
    "At each iteration $t$, Adam performs the following updates:\n",
    "\n",
    "1.  Compute the Gradients: Calculate the gradient $g_t$ at the current step.\n",
    "2.  Update the First Moment (Mean of Gradients):\n",
    "    \\begin{align}\n",
    "    m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\n",
    "    \\end{align}\n",
    "3.  Update the Second Moment (Mean of Squared Gradients):\n",
    "    \\begin{align}\n",
    "    v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\n",
    "    \\end{align}\n",
    "4.  Bias Correction: To account for initialization bias in the first few steps, Adam applies bias correction to both  $m_t$ and $v_t$:\n",
    "    \\begin{align}\n",
    "    \\hat{m}_t &= \\frac{m_t}{1 - \\beta_1^t} \\\\\n",
    "    \\hat{v}_t &= \\frac{v_t}{1 - \\beta_2^t}\n",
    "    \\end{align}\n",
    "5.  Parameter Update:\n",
    "    \\begin{align}\n",
    "    x_{t+1} = x_t - \\frac{\\alpha}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t\n",
    "    \\end{align}\n",
    "\n",
    "Here, $\\alpha$ is the learning rate, $\\epsilon$ is a small constant to prevent division by zero, and $\\beta_1$ and  $\\beta_2$ are decay rates for the first and second moments, typically set to 0.9 and 0.999, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam Optimizer\n",
    "def adam_hist(f, X0, alpha, beta1=0.9, beta2=0.999, epsilon=1e-8, imax=1000):\n",
    "    df = jit(grad(f))  # Use JAX to compute gradient\n",
    "    Xs = [X0]\n",
    "    M  =  0  # Initialize first moment\n",
    "    V  =  0  # Initialize second moment\n",
    "\n",
    "    for t in range(1, imax + 1): # use t instead of i to match the formulation\n",
    "        dfX = df(Xs[-1])  # Compute gradient\n",
    "        M   = beta1 * M + (1 - beta1) *  dfX      # Update biased first  moment estimate\n",
    "        V   = beta2 * V + (1 - beta2) * (dfX**2)  # Update biased second moment estimate\n",
    "        Mdb = M / (1 - beta1**t)  # debias first moment\n",
    "        Vdb = V / (1 - beta2**t)  # debias second moment\n",
    "        Xs.append(Xs[-1] - alpha * Mdb / (jnp.sqrt(Vdb) + epsilon))  # Update parameters\n",
    "\n",
    "    return Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for gradient descent\n",
    "C0    = jnp.zeros(len(groundtruth)) # Start with zeros as initial coefficients\n",
    "alpha = 0.1                         # Learning rate\n",
    "\n",
    "Cs = adam_hist(chi2, C0, alpha)\n",
    "%timeit -r1 Cs = adam_hist(chi2, C0, alpha)\n",
    "\n",
    "print(\"Optimized coefficients:\", Cs[-1])\n",
    "print(\"True coefficients:\",      groundtruth)\n",
    "print(\"Mean Squared Error:\",     np.mean((groundtruth - Cs[-1])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkplot(Xdata, Ydata, Ytrue, Cs, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "```{exercise}\n",
    "### Program Adam for Stochastic Gradient Descent\n",
    "\n",
    "Adam combines the strengths of adaptive learning rates and momentum, making it robust and effective for a variety of optimization tasks. Some key advantages include:\n",
    "\n",
    "* Efficient Parameter Updates: By adapting the learning rate per parameter, Adam handles sparse gradients and complex loss surfaces effectively.\n",
    "* Faster Convergence: Adam's use of momentum allows it to converge more quickly than standard SGD, particularly in cases with noisy gradients.\n",
    "* Stable Optimization: The algorithm’s adaptive approach reduces the risk of overshooting or oscillations, leading to more stable convergence.\n",
    "\n",
    "Adam is widely used in machine learning and deep learning because it combines the benefits of SGD with momentum and adaptive learning rates, making it more robust to the noise and non-convexity typical of high-dimensional models.\n",
    "Its efficiency and stability have made it the default optimizer in many neural network libraries, enabling practitioners to train models faster and more reliably.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "Root finding and optimization are foundational tools that enable the analysis, modeling, and solution of complex problems across many fields.\n",
    "While root finding focuses on solving equations where a function reaches zero, optimization aims to find the extrema of a function.\n",
    "The concepts are interconnected, as root finding is often used in optimization, and optimization problems can sometimes be reframed as root-finding tasks.\n",
    "\n",
    "Together, these techniques empower researchers, engineers, and data scientists to solve practical problems, from designing efficient systems to training predictive models.\n",
    "With a strong understanding of both root finding and optimization, you have the tools to approach a wide range of computational challenges, leveraging the power of numerical analysis to uncover insights and drive innovations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
