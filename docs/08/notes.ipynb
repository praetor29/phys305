{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Numerical Integration of Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Many physical laws are expressed as differential equations.\n",
    "For example, Newton's second law,\n",
    "\\begin{align}\n",
    "F = m \\frac{d^2 x}{dt^2},\n",
    "\\end{align}\n",
    "describes how a mass accelerates under a force.\n",
    "More complex phenomena in fluid flow, electromagnetism, or quantum mechanics involve partial differential equations (PDEs).\n",
    "Solving these usually requires integration over space, time, or both.\n",
    "Because most real-world problems cannot be solved analytically, numerical methods are essential for approximating their solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Numerical Integration of Functions\n",
    "\n",
    "To understand how to solve differential equations numerically, it helps to start with a simpler case: approximating a definite integral,\n",
    "\\begin{align}\n",
    "I = \\int_a^b f(x) \\, dx.\n",
    "\\end{align}\n",
    "This can be viewed as solving the initial-value problem\n",
    "\\begin{align}\n",
    "\\frac{dy}{dx} = f(x), \\quad y(a) = 0,\n",
    "\\end{align}\n",
    "where $y(b)$ represents the value of $\\int_a^b f(x)\\,dx$.\n",
    "By studying basic numerical integration, we learn how approximations converge to the true value when we refine parameters such as step size.\n",
    "These concepts of convergence and error control are crucial in more complex scenarios involving ordinary and partial differential equations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Analytical Example\n",
    "\n",
    "To illustrate these ideas, consider a function whose integral we know: $f(x) = e^x$.\n",
    "Its definite integral from $a$ to $b$ is\n",
    "\\begin{align}\n",
    "\\int_a^b f(x) \\, dx = e^b - e^a.\n",
    "\\end{align}\n",
    "We will use the interval $[0,1]$ so that the exact value is $e - 1$.\n",
    "Comparing this known result to numerical approximations will help us validate our methods and understand how accuracy depends on step size.\n",
    "\n",
    "Below is a simple plot of $e^x$ on $[0,1]$ for visual reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Define the function\n",
    "def f(x):\n",
    "    return np.exp(x)\n",
    "\n",
    "# Define a fine grid for plotting\n",
    "x = np.linspace(0, 1, 129)\n",
    "y = f(x)\n",
    "\n",
    "# Plotting the function\n",
    "plt.plot(x, y)\n",
    "plt.fill_between(x, y, alpha=0.33)\n",
    "plt.title(r'Plot of $f(x) = e^x$ on [0,1]')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Riemann Sums\n",
    "\n",
    "The Riemann sum is a foundational idea even in calculus.\n",
    "It partitions the interval $[a, b]$ into $n$ sub-intervals, each of width $\\Delta x_i$. The approximate integral is:\n",
    "\\begin{align}\n",
    "I \\approx \\sum_{i=1}^n f(x_i) \\Delta x_i,\n",
    "\\end{align}\n",
    "where $x_i$ is a sample point in the $i$-th sub-interval.\n",
    "Three common variations are:\n",
    "- Left Riemann Sum: Sample at the left endpoint.\n",
    "- Right Riemann Sum: Sample at the right endpoint.\n",
    "- Middle Riemann Sum: Sample at the midpoint.\n",
    "\n",
    "As the sub-interval width $\\Delta x_i$ goes to zero (i.e., as $n\\to\\infty$), each Riemann sum converges to the exact integral, assuming $f$ is reasonably well behaved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a coarse grid for visualization\n",
    "X = np.linspace(0, 1, 9)\n",
    "Y = f(X)\n",
    "\n",
    "# Plot Left Riemann Sum\n",
    "plt.plot(x, y)\n",
    "plt.scatter(X[:-1], Y[:-1], color='r')\n",
    "plt.fill_between(X, Y, step='post', color='r', alpha=0.33)\n",
    "plt.title('Left Riemann Sum for $f(x) = e^x$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Right Riemann Sum\n",
    "plt.plot(x, y)\n",
    "plt.scatter(X[1:], Y[1:], color='r')\n",
    "plt.fill_between(X, Y, step='pre', color='r', alpha=0.33)\n",
    "plt.title('Right Riemann Sum for $f(x) = e^x$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Middle Riemann Sum\n",
    "X_mid = 0.5 * (X[:-1] + X[1:])\n",
    "Y_mid = f(X_mid)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.scatter(X_mid, Y_mid, color='r')\n",
    "plt.fill_between(np.concatenate([[0], X_mid, [1]]),\n",
    "                 np.concatenate([Y_mid[:1], Y_mid, Y_mid[-1:]]),\n",
    "                 step='mid', color='r', alpha=0.33)\n",
    "plt.title('Middle Riemann Sum for $f(x) = e^x$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Computing Riemann Sums\n",
    "\n",
    "Next, letâ€™s implement each Riemann sum and compare their errors to the true value $I = e-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left Riemann Sum calculation\n",
    "N = 8\n",
    "D = 1 / N\n",
    "X_left = [D * i for i in range(N)]\n",
    "S_left = np.sum(f(X_left)) * D\n",
    "\n",
    "print('Left Riemann Sum:', S_left)\n",
    "\n",
    "# Exact solution\n",
    "I = f(1) - f(0)\n",
    "print('Analytical solution:', I)\n",
    "\n",
    "# Error analysis\n",
    "aerr_left = abs(I - S_left)\n",
    "rerr_left = abs((I - S_left) / I)\n",
    "print('Absolute error:', aerr_left)\n",
    "print(f'Relative error: {100 * rerr_left:.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right Riemann Sum calculation\n",
    "X_right = [D * (i + 1) for i in range(N)]\n",
    "S_right = np.sum(f(X_right) * D)\n",
    "\n",
    "print('Right Riemann Sum:', S_right)\n",
    "\n",
    "# Error analysis\n",
    "aerr_right = abs(I - S_right)\n",
    "rerr_right = abs((I - S_right) / I)\n",
    "print('Absolute error:', aerr_right)\n",
    "print(f'Relative error: {100 * rerr_right:.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Middle Riemann Sum calculation\n",
    "X_mid = [D * (i + 0.5) for i in range(N)]\n",
    "S_mid = np.sum(f(X_mid) * D)\n",
    "\n",
    "print('Middle Riemann Sum:', S_mid)\n",
    "\n",
    "# Error analysis\n",
    "aerr_mid = abs(I - S_mid)\n",
    "rerr_mid = abs((I - S_mid) / I)\n",
    "print('Absolute error:', aerr_mid)\n",
    "print(f'Relative error: {100 * rerr_mid:.2f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "We often find that the middle Riemann sum is more accurate than the left or right, even though each uses the same number of points.\n",
    "This illustrates how the choice of sample point can impact accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Convergence of Riemann Sums\n",
    "\n",
    "In numerical integration, convergence describes how closely a numerical approximation approaches the exact value of an integral as the number of sample points increases.\n",
    "To illustrate convergence, we will analyze the errors in different types of Riemann sums (left, right, and middle) as we increase the number of sampling points.\n",
    "\n",
    "To streamline our code, let's define a function `RiemannSum()` that allows us to compute the Riemann sum for any function $f$ using different sample points (left, right, or middle) and varying the number of intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General function for Riemann sum calculation\n",
    "def RiemannSum(f, N=8, a=0, b=1, t='mid'):\n",
    "    \"\"\"Compute Riemann sum for function $f$ from $a$ to $b$ using $N$ points.\n",
    "\n",
    "    Parameters:\n",
    "      f: function to be ingegrated\n",
    "      N: number of subdivision\n",
    "      a: lower limit\n",
    "      b: upper limit\n",
    "      t: type of Riemann sum, can be `mid` (middle), `left`, or `right`.\n",
    "    \"\"\"\n",
    "    D = (b - a) / N  # Step size\n",
    "    if t[0] == 'l':\n",
    "        X = [D * i + a for i in range(N)]\n",
    "    elif t[0] == 'r':\n",
    "        X = [D * (i + 1) + a for i in range(N)]\n",
    "    else:\n",
    "        X = [D * (i + 0.5) + a for i in range(N)]\n",
    "    return np.sum(f(np.array(X))) * D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "We'll use varying numbers of sampling points $N$ and compute the absolute error for each Riemann sum type compared to the true integral value.\n",
    "Below, we test this with the function $f(x) = e^x$ again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define range of sampling points\n",
    "Ns = np.array([4, 8, 16, 32, 64, 128, 256, 512, 1024])\n",
    "\n",
    "# True value of the integral for comparison\n",
    "I = np.exp(1) - 1\n",
    "\n",
    "# Calculate errors for left, middle, and right Riemann sums\n",
    "err_l = [abs(RiemannSum(f, N, t='l') - I) for N in Ns]\n",
    "err_m = [abs(RiemannSum(f, N, t='m') - I) for N in Ns]\n",
    "err_r = [abs(RiemannSum(f, N, t='r') - I) for N in Ns]\n",
    "\n",
    "# Plotting the convergence results\n",
    "plt.loglog(Ns, err_l, '+--', color='r',  label='Left Riemann Sum')\n",
    "plt.loglog(Ns, err_m, 'o-',  color='C0', label='Middle Riemann Sum')\n",
    "plt.loglog(Ns, err_r, 'x--', color='g',  label='Right Riemann Sum')\n",
    "plt.loglog(Ns, Ns**(-1.0), ':', lw=0.5, label=r'$N^{-1}$')\n",
    "plt.loglog(Ns, Ns**(-2.0), ':', lw=0.5, label=r'$N^{-2}$')\n",
    "plt.xlabel('Number of Sampling Points')\n",
    "plt.ylabel('Absolute Error')\n",
    "plt.legend()\n",
    "plt.title('Convergence of Riemann Sums for $f(x) = e^x$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "From this plot, we see that:\n",
    "- Left and Right Riemann sums typically converge at a rate proportional to $N^{-1}$.\n",
    "- Middle Riemann sum often achieves about $N^{-2}$ convergence, making it more accurate at lower $N$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Testing Convergence with Different Functions\n",
    "\n",
    "To determine if this trend holds generally, letâ€™s repeat the convergence test with different functions: a half-cycle of $\\sin(x)$ and a quarter circle $\\sqrt{1 - x^2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a half-cycle of sin(x)\n",
    "def g(x):\n",
    "    return np.sin(x * np.pi / 2)\n",
    "\n",
    "# Calculate errors for each Riemann sum type\n",
    "err_l = [abs(RiemannSum(g, N, t='l') - 2 / np.pi) for N in Ns]\n",
    "err_m = [abs(RiemannSum(g, N, t='m') - 2 / np.pi) for N in Ns]\n",
    "err_r = [abs(RiemannSum(g, N, t='r') - 2 / np.pi) for N in Ns]\n",
    "\n",
    "# Plotting the convergence results\n",
    "plt.loglog(Ns, err_l, '+--', color='r',  label='Left Riemann Sum')\n",
    "plt.loglog(Ns, err_m, 'o-',  color='C0', label='Middle Riemann Sum')\n",
    "plt.loglog(Ns, err_r, 'x--', color='g',  label='Right Riemann Sum')\n",
    "plt.loglog(Ns, Ns**(-1.0), ':', lw=0.5, label=r'$N^{-1}$')\n",
    "plt.loglog(Ns, Ns**(-2.0), ':', lw=0.5, label=r'$N^{-2}$')\n",
    "plt.xlabel('Number of Sampling Points')\n",
    "plt.ylabel('Absolute Error')\n",
    "plt.legend()\n",
    "plt.title(r'Convergence of Riemann Sums for $g(x) = \\sin(\\pi x / 2)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(x):\n",
    "    return np.sqrt(1 - x * x)\n",
    "\n",
    "X = np.linspace(0, 1, 11)\n",
    "X = 0.5 * (X[:-1] + X[1:])\n",
    "Y = h(X)\n",
    "\n",
    "plt.plot(x, h(x))\n",
    "plt.scatter(X, Y, color='r')\n",
    "plt.fill_between(np.concatenate([[0], X, [1]]),\n",
    "                 np.concatenate([Y[:1], Y, Y[-1:]]),\n",
    "                 step='mid', color='r', alpha=0.33)\n",
    "plt.gca().set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a quarter circle function\n",
    "def h(x):\n",
    "    return np.sqrt(1 - x**2)\n",
    "\n",
    "# Calculate errors for each Riemann sum type\n",
    "err_l = [abs(RiemannSum(h, N, t='l') - np.pi / 4) for N in Ns]\n",
    "err_m = [abs(RiemannSum(h, N, t='m') - np.pi / 4) for N in Ns]\n",
    "err_r = [abs(RiemannSum(h, N, t='r') - np.pi / 4) for N in Ns]\n",
    "\n",
    "# Plotting the convergence results\n",
    "plt.loglog(Ns, err_l, '+--', color='r',  label='Left Riemann Sum')\n",
    "plt.loglog(Ns, err_m, 'o-',  color='C0', label='Middle Riemann Sum')\n",
    "plt.loglog(Ns, err_r, 'x--', color='g',  label='Right Riemann Sum')\n",
    "plt.loglog(Ns, Ns**(-1.0), ':', lw=0.5, label=r'$N^{-1}$')\n",
    "plt.loglog(Ns, Ns**(-2.0), ':', lw=0.5, label=r'$N^{-2}$')\n",
    "plt.xlabel('Number of Sampling Points')\n",
    "plt.ylabel('Absolute Error')\n",
    "plt.legend()\n",
    "plt.title(r'Convergence of Riemann Sums for $h(x) = \\sqrt{1 - x^2}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Although the specific error values vary across different functions, the general trends in convergence remain consistent.\n",
    "Doubling the number of sampling points, or equivalently halving the step size, reduces the error in the left and right Riemann sums by roughly half.\n",
    "By comparison, the middle Riemann sum achieves a fourfold reduction in error for each doubling of sampling points, indicating a faster rate of convergence.\n",
    "This suggests that, overall, the middle Riemann sum converges more rapidly than the left and right sums.\n",
    "\n",
    "As we move forward, we will adopt the notation and methods used in Numerical Recipes. These approaches will provide greater flexibility and accuracy, enabling us to tackle more complex integration problems more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "In general, we observe that:\n",
    "* For the left and right Riemann sums, doubling the number of sampling points halves the error. This is an example of **first-order convergence**.\n",
    "* For the middle Riemann sum, doubling the number of sampling points reduces the error by a factor of four, indicating **second-order convergence**.\n",
    "\n",
    "However, these formal convergent rates may not hold for specific functions $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Classical Formulas for Equally Spaced Abscissas\n",
    "\n",
    "### Trapezoidal Rule\n",
    "\n",
    "To improve our numerical integration, we will transition from the Riemann sum approach to the **trapezoidal rule**.\n",
    "From now on, we'll adopt a \"vertex\" formulation for points, where $x_i = a + i \\Delta x$ and $\\Delta x = (b - a)/N$.\n",
    "\n",
    "The trapezoidal rule approximates the area under a curve by treating each segment as a trapezoid.\n",
    "For a single interval $[x_0, x_1]$, we approximate:\n",
    "\\begin{align}\n",
    "\\int_{x_0}^{x_1} f(x) \\, dx \\approx h \\left( \\frac{1}{2} f(x_0) + \\frac{1}{2} f(x_1) \\right) + \\mathcal{O}(h^3 f'')\n",
    "\\end{align}\n",
    "where $h = x_1 - x_0$.\n",
    "This error term indicates that the true value differs by an amount proportional to $h^3$ and $f''$, making it second-order accurate.\n",
    "If $f$ is linear (i.e., $f'' = 0$), the trapezoidal approximation is exact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different functions, this is a quarter circle\n",
    "\n",
    "X = np.linspace(0, 1, 3)\n",
    "Y = g(X)\n",
    "\n",
    "plt.plot(x, g(x))\n",
    "plt.scatter(X, Y, color='r')\n",
    "plt.fill_between(X, g(X), color='r', alpha=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trapezoidal rule implementation\n",
    "def trapezoidal(f, N=8, a=0, b=1):\n",
    "    X, D = np.linspace(a, b, N+1, retstep=True)\n",
    "    return np.sum(f(X[1:]) + f(X[:-1])) * 0.5 * D\n",
    "\n",
    "# Compare errors of middle Riemann sum and trapezoidal rule\n",
    "err_m = [abs(RiemannSum(g, N, t='m') - 2 / np.pi) for N in Ns]\n",
    "err_t = [abs(trapezoidal(g, N) - 2 / np.pi) for N in Ns]\n",
    "\n",
    "plt.loglog(Ns, err_m, 'o--', color='C0', label='Middle Riemann Sum')\n",
    "plt.loglog(Ns, err_t, '+-',  color='C1', label='Trapezoidal Rule')\n",
    "plt.loglog(Ns, Ns**(-2.0), ':', lw=0.5, label=r'$N^{-2}$')\n",
    "plt.xlabel('Number of Sampling Points')\n",
    "plt.ylabel('Absolute Error')\n",
    "plt.legend()\n",
    "plt.title('Error Comparison of Middle Riemann Sum and Trapezoidal Rule')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### Simpson's Rule\n",
    "\n",
    "The trapezoidal rule is exact for linear functions.\n",
    "Naturally, we may wonder if there's a method exact for quadratic functions.\n",
    "This leads us to Simpson's Rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "There are many ways to derive the Simpson's Rule.\n",
    "For simplicity, let's \"center\" the integral at 0 and set the lower and upper limits to $-h$ and $h$.\n",
    "The integrate to approximated is therefore:\n",
    "\\begin{align}\n",
    "\\int_{-h}^h f(x) dx \n",
    "&\\approx \\int_{-h}^h (Ax^2 + Bx + C) dx \\\\\n",
    "&= \\left[A\\frac{x^3}{3} + B\\frac{x^2}{2} + Cx\\right]_{-h}^h \\\\\n",
    "&= 2h\\left(A\\frac{h^2}{3} + C\\right)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "The coefficient $A$ and $C$ can be trivial obtain by noting:\n",
    "\\begin{align}\n",
    "f( h) &= Ah^2 + Bh + C, \\\\\n",
    "f( 0) &= C, \\\\\n",
    "f(-h) &= Ah^2 - Bh + C.\n",
    "\\end{align}\n",
    "Adding the first and third equations,\n",
    "\\begin{align}\n",
    "f(h) + f(-h) = 2(A h^2 + C) \\Rightarrow A h^2 = \\frac{f(h) + f(-h)}{2} - C.\n",
    "\\end{align}\n",
    "Substituting $A$ and $C$ back to the integral, we have\n",
    "\\begin{align}\n",
    "\\int_{-h}^h f(x) dx \\approx \\frac{h}{3}[f(h) + 4 f(0) + f(-h)].\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "Restoring the limits, Simpson's rule approximates the integral over two intervals $[x_0, x_2]$ yields:\n",
    "\\begin{align}\n",
    "\\int_{x_0}^{x_2} f(x) \\, dx \\approx \\frac{h}{3} \\left[f(x_0) + 4f(x_1) + f(x_2)\\right] + \\mathcal{O}(h^5 f^{(4)})\n",
    "\\end{align}\n",
    "where $h = \\frac{x_2 - x_0}{2}$ and $x_1 = \\frac{x_0 + x_2}{2}$.\n",
    "This error term indicates that Simpson's Rule is fourth-order accurate---even when using quadratic approximations, we achieve convergence as though we were using fourth-degree terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpson's rule implementation\n",
    "def simpson(f, N=8, a=0, b=1):\n",
    "    X, D = np.linspace(a, b, N+1, retstep=True)\n",
    "    S = 0\n",
    "    for i in range(N // 2):\n",
    "        l = X[2 * i]\n",
    "        m = X[2 * i + 1]\n",
    "        r = X[2 * i + 2]\n",
    "        S += D * (f(l) + 4 * f(m) + f(r)) / 3\n",
    "    return S\n",
    "\n",
    "# Compare errors of middle Riemann, trapezoidal, and Simpson's rule\n",
    "err_S = [abs(simpson(g, N) - 2 / np.pi) for N in Ns]\n",
    "\n",
    "plt.loglog(Ns, err_m, 'o--', color='C0', label='Middle Riemann Sum')\n",
    "plt.loglog(Ns, err_t, '+--', color='C1', label='Trapezoidal Rule')\n",
    "plt.loglog(Ns, err_S, 'x-',  color='C2', label=\"Simpson's Rule\")\n",
    "plt.loglog(Ns, Ns**(-2.0), ':', lw=0.5, label=r'$N^{-2}$')\n",
    "plt.loglog(Ns, Ns**(-4.0), ':', lw=0.5, label=r'$N^{-4}$')\n",
    "plt.xlabel('Number of Sampling Points')\n",
    "plt.ylabel('Absolute Error')\n",
    "plt.legend()\n",
    "plt.title('Error Comparison of Middle Riemann, Trapezoidal, and Simpsonâ€™s Rules')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### Bode's Rule\n",
    "\n",
    "Simpson's rule is exact for quadratic polynomials, but what if we want a rule that is exact for quartic polynomials?\n",
    "Bode's Rule addresses this, using a polynomial fit across four intervals to achieve even higher accuracy.\n",
    "\n",
    "Bode's rule integrates over four intervals, providing an approximation that is exact for polynomials up to degree four:\n",
    "\\begin{align}\n",
    "\\int_{x_0}^{x_4} f(x) , dx \\approx h \\left( \\frac{14}{45} f(x_0) + \\frac{64}{45} f(x_1) + \\frac{24}{45} f(x_2) + \\frac{64}{45} f(x_3) + \\frac{14}{45} f(x_4) \\right) + \\mathcal{O}(h^7 f^{(6)})\n",
    "\\end{align}\n",
    "where $h = (x_4 - x_0)/4$.\n",
    "This method is sixth-order accurate, meaning it converges more quickly than Simpson's Rule for smooth functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bode's rule implementation\n",
    "def bode(f, N=8, a=0, b=1):\n",
    "    X, D = np.linspace(a, b, N+1, retstep=True)\n",
    "    S = 0\n",
    "    for i in range(N // 4):\n",
    "        x0 = X[4 * i]\n",
    "        x1 = X[4 * i + 1]\n",
    "        x2 = X[4 * i + 2]\n",
    "        x3 = X[4 * i + 3]\n",
    "        x4 = X[4 * i + 4]\n",
    "        S += D * (14 * f(x0) + 64 * f(x1) + 24 * f(x2) + 64 * f(x3) + 14 * f(x4)) / 45\n",
    "    return S\n",
    "\n",
    "# Compare errors of middle Riemann, trapezoidal, Simpson's, and Bode's rule\n",
    "err_B = [abs(bode(g, N) - 2 / np.pi) for N in Ns]\n",
    "\n",
    "plt.loglog(Ns, err_m, 'o--',  color='C0', label='Middle Riemann Sum')\n",
    "plt.loglog(Ns, err_t, '+--',  color='C1', label='Trapezoidal Rule')\n",
    "plt.loglog(Ns, err_S, 'x--',  color='C2', label=\"Simpson's Rule\")\n",
    "plt.loglog(Ns, err_B, 'o-',   color='C3', label=\"Bode's Rule\")\n",
    "plt.loglog(Ns, Ns**(-2.0), ':', lw=0.5, label=r'$N^{-2}$')\n",
    "plt.loglog(Ns, Ns**(-4.0), ':', lw=0.5, label=r'$N^{-4}$')\n",
    "plt.loglog(Ns, Ns**(-6.0), ':', lw=0.5, label=r'$N^{-6}$')\n",
    "plt.xlabel('Number of Sampling Points')\n",
    "plt.ylabel('Absolute Error')\n",
    "plt.legend()\n",
    "plt.title('Error Comparison of Middle Riemann, Trapezoidal, Simpsonâ€™s, and Bodeâ€™s Rules')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## Gaussian Quadrature\n",
    "\n",
    "In previous lectures, we went through numerical methods that uses high-order polynomial to approximate a function.\n",
    "These high-order methods enable exponential convergence for interpolation and derivatives.\n",
    "\n",
    "Exponentially converging methods also exists for numerical integration.\n",
    "They are called Gaussian quadratures.\n",
    "Unlike methods such as the trapezoidal or Simpson's rule, which evaluate the function at equally spaced points, Gaussian quadrature selects **optimal** points and corresponding weights to achieve exact integration for polynomials up to a certain degree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "To make the derivation easier to follow, we set the interval of integration to $[-1,1]$.\n",
    "The key idea behind Gaussian quadrature is to approximate an integral of the form:\n",
    "\\begin{align}\n",
    "I = \\int_{-1}^{1} f(x)\\,dx\n",
    "\\end{align}\n",
    "by a weighted sum:\n",
    "\\begin{align}\n",
    "I \\approx \\sum_{i=1}^{n} w_i f(x_i),\n",
    "\\end{align}\n",
    "where the nodes $x_i$ and weights $w_i$ are carefully chosen to maximize accuracy.\n",
    "\n",
    "Note that the classical formulas introduced above are special cases with equidistant sampling points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "Gaussian quadrature is constructed such that it **exactly** integrates polynomials of degree up to $2n - 1$ using only $n$ function evaluations.\n",
    "To derive the optimal nodes and weights, we enforce this exactness condition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "The optimal nodes $x_i$ in Gaussian quadrature are chosen as the **roots of the Legendre polynomial** $P_n(x)$, where $P_n(x)$ is the $n$th-degree Legendre polynomial.\n",
    "These polynomials satisfy the **orthogonality relation**:\n",
    "\\begin{align}\n",
    "\\int_{-1}^{1} P_m(x) P_n(x) \\, dx = 0 \\quad \\text{for } m \\neq n.\n",
    "\\end{align}\n",
    "This orthogonality property plays a crucial role in minimizing the integration error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "Once the nodes $x_i$ are determined, the weights $w_i$ are found using the formula:\n",
    "\\begin{align}\n",
    "w_i = \\frac{2}{(1 - x_i^2) [P_n'(x_i)]^2}.\n",
    "\\end{align}\n",
    "Here, $P_n'(x_i)$ is the derivative of the Legendre polynomial evaluated at the root $x_i$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "Although Gaussian quadrature is typically defined on the standard interval $[-1,1]$.\n",
    "To integrate over an arbitrary interval $[a, b]$, we simply apply the transformation:\n",
    "\\begin{align}\n",
    "t = \\frac{b-a}{2} x + \\frac{b+a}{2}.\n",
    "\\end{align}\n",
    "This maps the integration from $[-1,1]$ to $[a, b]$, introducing a factor of $\\frac{b-a}{2}$ in the integral:\n",
    "\\begin{align}\n",
    "\\int_a^b f(t) dt = \\frac{b-a}{2} \\int_{-1}^{1} f\\left( \\frac{b-a}{2} x + \\frac{b+a}{2} \\right) dx.\n",
    "\\end{align}\n",
    "Applying Gaussian quadrature, the integral is approximated as:\n",
    "\\begin{align}\n",
    "\\int_a^b f(x) dx \\approx \\frac{b-a}{2} \\sum_{i=1}^{n} w_i f\\left( \\frac{b-a}{2} x_i + \\frac{b+a}{2} \\right).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "To implement Gaussian Quadrature in python, we will use `numpy.polynomial.legendre.leggauss` to obtain the Gaussian quadrature nodes and weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GQ(f, a, b, n):\n",
    "    \"\"\"\n",
    "    Perform Gaussian quadrature on function f over interval [a, b] using n points.\n",
    "    \"\"\"\n",
    "    # Get nodes and weights for Legendre polynomials\n",
    "    x, w = np.polynomial.legendre.leggauss(n)\n",
    "    \n",
    "    # Transform nodes from [-1,1] to [a,b]\n",
    "    t = 0.5 * (b - a) * x + 0.5 * (b + a)\n",
    "    \n",
    "    # Compute weighted sum\n",
    "    return 0.5 * (b - a) * np.sum(w * f(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare errors of middle Riemann, trapezoidal, Simpson's, and Bode's rule\n",
    "err_GQ = [abs(GQ(g, 0, 1, N) - 2 / np.pi) for N in Ns]\n",
    "\n",
    "plt.loglog(Ns, err_m, 'o--',  color='g',  label='Middle Riemann Sum')\n",
    "plt.loglog(Ns, err_t, '+--',  color='r',  label='Trapezoidal Rule')\n",
    "plt.loglog(Ns, err_S, 'x--',  color='b',  label=\"Simpson's Rule\")\n",
    "plt.loglog(Ns, err_B, 'o--',  color='k',  label=\"Bode's Rule\")\n",
    "plt.loglog(Ns, err_GQ,'.-',   color='k',  label=\"Gaussian quadrature\")\n",
    "plt.loglog(Ns, Ns**(-2.0), ':', lw=0.5, label=r'$N^{-2}$')\n",
    "plt.loglog(Ns, Ns**(-4.0), ':', lw=0.5, label=r'$N^{-4}$')\n",
    "plt.loglog(Ns, Ns**(-6.0), ':', lw=0.5, label=r'$N^{-6}$')\n",
    "plt.xlabel('Number of Sampling Points')\n",
    "plt.ylabel('Absolute Error')\n",
    "plt.legend()\n",
    "plt.title('Error Comparison of Middle Riemann, Trapezoidal, Simpsonâ€™s, and Bodeâ€™s Rules')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "## Using Scipy and Sympy\n",
    "\n",
    "Instead of implementing our own numerical schemes, in real research projects, it is more likely that you will use a state-of-art integrator from a well maintained package.\n",
    "\n",
    "For numerical integration, `scipy` has a recently collection of integrators.\n",
    "\n",
    "For symbolic integration, `sympy` is the standard choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scipy example for numerical integration\n",
    "\n",
    "from scipy.integrate import quad\n",
    "\n",
    "res, err = quad(lambda x: np.sqrt(1 - x * x), 0, 1)\n",
    "\n",
    "print('Result:', res)\n",
    "print('Error: ', err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sympy example for symbolic integration\n",
    "\n",
    "from sympy import Symbol, integrate, sqrt\n",
    "\n",
    "x = Symbol('x')\n",
    "\n",
    "integrate(sqrt(1 - x * x), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "integrate(sqrt(1 - x * x), (x, 0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "## Final comments\n",
    "\n",
    "* Based on the previous examples, by increasing the order of the approximations, it is possible to construct numerical integration that converges very rapidly.\n",
    "\n",
    "* For double precision floating point values, the machine accruacy is $\\sim 10^{-16}$.  We saw with Bode's rule, we are already reaching that limit for $\\sim 256$ sampling points.\n",
    "\n",
    "* For smooth functions, it is even possible to develop numerical integrators such as Gaussian quadrature that converge exponentially!\n",
    "\n",
    "* Symbolic integration provided by, e.g., `sympy`, can also be a useful tool.\n",
    "\n",
    "* However, if the function is not smooth, i.e., with discontinuity, then formally the convergent rate is only first order.\n",
    "  Hence, refining the sampling points near the discontinuity is the only method to provide accurate integration.\n",
    "\n",
    "* Also, the approximations we introduce in this lecture includes the *end points* of the function.\n",
    "  It will be difficult to apply these numerical methods to, e.g., improper integral, or functions with singularity.\n",
    "\n",
    "* In the lab, we will learn how to modify our integrators to exclude the end points, and use them for improper integral and functions with singularity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
